# ğŸ•·ï¸ Site Crawler

A powerful website crawler built with [Playwright](https://playwright.dev/), designed to recursively scan websites, detect broken links and placeholder text, and generate detailed reports in multiple formats.

---

## ğŸš€ Features

- ğŸ”— **Link Validation** â€“ Detects broken, redirected, or absolute internal links
- ğŸ§  **Placeholder Detection** â€“ Finds common filler text and templated content
- âš™ï¸ **SEO Checks** â€“ Optionally check titles, meta descriptions, and heading structure
- ğŸ“¸ **Image Auditing** â€“ Find images missing `alt` text
- â˜ï¸ **Tel Link Validation** â€“ Verify `tel:` links for correct country code and format
- ğŸ“ˆ **Report Generation** â€“ Outputs results in JSON, CSV, and HTML formats
- ğŸŒ **Sitemap Support** â€“ Start crawling from a sitemap if configured

---

## ğŸ“ Project Structure

```
site-crawler/
â”œâ”€â”€ crawl-results/          # Output folder for reports
â”‚   â””â”€â”€ .gitkeep            # Ensures the folder is committed but remains empty
â”œâ”€â”€ config.template.js      # Sample config file
â”œâ”€â”€ config.js               # Your actual config file (ignored by Git)
â”œâ”€â”€ crawler.js              # Main crawler script
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

The crawler uses a local `config.js` file. A sample is provided as `config.template.js`.

### ğŸ“ Create Your Config

```bash
cp config.template.js config.js
```

### Example Configuration

```js
module.exports = {
  startUrl: "https://example.com/sitemap",
  maxDepth: 4,
  timeout: 10000,
  outputDir: "./crawl-results",
  useSitemap: true,
  skipRepeatedNavLinks: true,
  navSelector: "nav a[href]",
  requestDelay: 1000,
  placeholders: ["Lorem ipsum", "000.000.0000", "ADDRESS", "CITY", "ZIP"],
  excludeUrls: ["https://www.facebook.com/"],
  excludePatterns: ["login", "logout"],
  checks: {
    title: true,
    httpStatus: false,
    seo: false,
    placeholders: true,
    links: true,
    performance: false,
    images: false,
    telLinks: true
  },
  telCountryCodes: ["+1"]
};
```

---

## ğŸ“† Installation

```bash
npm install
```

---

## â–¶ï¸ Running the Crawler

```bash
node crawler.js
```

Interrupt with `Ctrl+C` to stop and save results early.

---

## ğŸ“‚ Output Files

Crawl results are saved to the `crawl-results/` folder:

| File | Description |
|------|-------------|
| `crawl-report-*.json` | Structured crawl results |
| `issues-*.csv` | Spreadsheet-friendly report |
| `report-*.html` | Visual HTML report |
| `site-graph-*.json` | Page-to-page link graph |

---

## ğŸ™ˆ .gitignore Highlights

```gitignore
# Ignore dependencies and temp files
node_modules/
.vscode/
*.log

# Ignore personal config and results
config.js
crawl-results/*
!crawl-results/.gitkeep
```

---

## ğŸ’¡ Tips

- Customize `placeholder` terms for your brand or CMS.
- Use in CI pipelines for automated link/SEO validation.
- Run on staging environments before production release.

---

## ğŸ“„ License

MIT â€” free to use, modify, and contribute.

---

## ğŸ¤ Contributions

Pull requests and suggestions are welcome! Open an issue to discuss major changes.

---

## âœ‰ï¸ Author

Built with â¤ï¸ by Nathaniel Blazier

